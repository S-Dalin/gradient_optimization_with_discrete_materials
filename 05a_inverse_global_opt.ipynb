{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global-based Inverse Design\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pymiecs\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the data scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/newData_122500/04_shuffle_features_scaler_MinMax(-1,1)_OneHot.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m scaler_Qback_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets/newData_122500/04_shuffle_Qback_target_MinMaxScaler_-1-1.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Load the preprocessors and scalers\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreprocessor_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     preprocessor \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(scaler_Qfwd_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/.pykeras/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'datasets/newData_122500/04_shuffle_features_scaler_MinMax(-1,1)_OneHot.pkl'"
     ]
    }
   ],
   "source": [
    "# preprocessor path\n",
    "preprocessor_path = \"datasets/04_shuffle_features_scaler_MinMax(-1,1)_OneHot.pkl\"\n",
    "scaler_Qfwd_path = \"datasets/04_shuffle_Qfwd_target_MinMaxScaler_-1-1.pkl\"\n",
    "scaler_Qback_path = \"datasets/newData_122500/04_shuffle_Qback_target_MinMaxScaler_-1-1.pkl\"\n",
    "\n",
    "\n",
    "# Load the preprocessors and scalers\n",
    "with open(preprocessor_path, \"rb\") as f:\n",
    "    preprocessor = pickle.load(f)\n",
    "with open(scaler_Qfwd_path, \"rb\") as f:\n",
    "    scaler_Qfwd = pickle.load(f)\n",
    "with open(scaler_Qback_path, \"rb\") as f:\n",
    "    scaler_Qback = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mie Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% --- Mie\n",
    "def get_Mie_spec(wavelengths, r_core, r_shell, mat_core, mat_shell, n_env):\n",
    "\n",
    "    k0 = 2 * np.pi / wavelengths\n",
    "    n_core = mat_core.get_refindex(wavelengths)\n",
    "    n_shell = mat_shell.get_refindex(wavelengths)\n",
    "\n",
    "    res = pymiecs.Q(\n",
    "        k0,\n",
    "        r_core=r_core,\n",
    "        n_core=n_core,\n",
    "        r_shell=r_shell,\n",
    "        n_shell=n_shell,\n",
    "        n_env=n_env.real**0.5,  # host medium must be lossless\n",
    "    )\n",
    "    return (\n",
    "        res['qsca'],\n",
    "        res['qback'],\n",
    "        res['qfwd'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Si = pymiecs.materials.MaterialDatabase('Si')\n",
    "SiO2 = pymiecs.materials.MaterialDatabase('SiO2')\n",
    "Si3N4 = pymiecs.materials.MaterialDatabase('Si3N4')\n",
    "Au = pymiecs.materials.MaterialDatabase('Au')\n",
    "Ag = pymiecs.materials.MaterialDatabase('Ag')\n",
    "ZrO2 = pymiecs.materials.MaterialDatabase('ZrO2')\n",
    "TiO2 = pymiecs.materials.MaterialDatabase('TiO2')\n",
    "\n",
    "\n",
    "# Define a function to map material names to material objects\n",
    "def get_material(material_name):\n",
    "    if material_name == 'Si':\n",
    "        return Si\n",
    "    elif material_name == 'SiO2':\n",
    "        return SiO2\n",
    "    elif material_name == 'Au':\n",
    "        return Au\n",
    "    elif material_name == 'Ag':\n",
    "        return Ag\n",
    "    elif material_name == 'Si3N4':\n",
    "        return Si3N4\n",
    "    elif material_name == 'ZrO2':\n",
    "        return ZrO2\n",
    "    elif material_name == 'TiO2':\n",
    "        return TiO2\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown material: {material_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reload Test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_df_file = \"datasets/newData_122500/02_h5_wide_122500_df_test_with_pred.h5\"   # depend on what test set that forward model evalaute on\n",
    "df_test = pd.read_hdf(hdf5_df_file)\n",
    "df_test.head() # 2500 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['log_Qfwd'] = df_test['Q_fwd'].apply(lambda x: np.log1p(np.array(x)))\n",
    "df_test['log_Qback'] = df_test['Q_back'].apply(lambda x: np.log1p(np.array(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mie_scattering_error(\n",
    "    r_core,\n",
    "    r_shell,\n",
    "    core_material,\n",
    "    shell_material,\n",
    "    target_Qfwd_transformed,\n",
    "    target_Qback_transformed,\n",
    "):\n",
    "    # Define wavelengths\n",
    "    wavelengths = np.linspace(400, 800, 64)\n",
    "\n",
    "    # Get material properties using your material function\n",
    "    mat_core = get_material(core_material)\n",
    "    mat_shell = get_material(shell_material)\n",
    "\n",
    "    # Calculate Mie scattering using the new package\n",
    "    Qsca, Qback, Qfwd = get_Mie_spec(\n",
    "        wavelengths=wavelengths,\n",
    "        r_core=r_core,\n",
    "        r_shell=r_shell,\n",
    "        mat_core=mat_core,\n",
    "        mat_shell=mat_shell,\n",
    "        n_env=1.0,  # Medium refractive index\n",
    "    )\n",
    "\n",
    "    # Apply log1p transformation to Mie Qfwd and Qback\n",
    "    log_mie_Qfwd = np.log1p(Qfwd)\n",
    "    log_mie_Qback = np.log1p(Qback)\n",
    "\n",
    "    # Now, apply MinMax scaling to log-transformed Mie values\n",
    "    log_mie_Qfwd_transformed = scaler_Qfwd.transform(\n",
    "        log_mie_Qfwd.reshape(1, -1)\n",
    "    )  # Reshape to (1, 64)\n",
    "    log_mie_Qback_transformed = scaler_Qback.transform(log_mie_Qback.reshape(1, -1))\n",
    "\n",
    "    # Compute the MSE between the predicted and target values\n",
    "    mse_Qfwd = np.mean((log_mie_Qfwd_transformed - target_Qfwd_transformed) ** 2)\n",
    "    mse_Qback = np.mean((log_mie_Qback_transformed - target_Qback_transformed) ** 2)\n",
    "\n",
    "    # Total error is the sum of MSEs\n",
    "    total_error = mse_Qfwd + mse_Qback\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function (your Mie calculations and comparison with target)\n",
    "def objective_function(\n",
    "    geometry_params, target_Qfwd_transformed, target_Qback_transformed\n",
    "):\n",
    "    # Your Mie calculations and error computation go here\n",
    "    r_core_real, r_shell_real, mat_core, mat_shell = geometry_params\n",
    "\n",
    "    # (Example calculation) Total error is the sum of MSEs for Qfwd and Qback\n",
    "    total_error = calculate_mie_scattering_error(\n",
    "        r_core_real,\n",
    "        r_shell_real,\n",
    "        mat_core,\n",
    "        mat_shell,\n",
    "        target_Qfwd_transformed,\n",
    "        target_Qback_transformed,\n",
    "    )\n",
    "\n",
    "    return total_error  # The optimizer will minimize this error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run global Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n",
    "from concurrent import futures\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search space for core radius, shell thickness, core material, shell material\n",
    "parametrization = ng.p.Tuple(\n",
    "    ng.p.Scalar(lower=1, upper=100),  # Real core radius (nm)\n",
    "    ng.p.Scalar(lower=6, upper=200),   # Real shell thickness (nm)\n",
    "    ng.p.Choice([\"Si\", \"SiO2\", \"Au\", \"Ag\", \"Si3N4\",\"ZrO2\", \"TiO2\"]),  # Core material\n",
    "    ng.p.Choice([\"Si\", \"SiO2\", \"Au\", \"Ag\", \"Si3N4\",\"ZrO2\", \"TiO2\"])   # Shell material\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store results and runtimes\n",
    "sample_runtimes = []\n",
    "results = []\n",
    "# Define the number of workers for parallel processing and budget\n",
    "num_workers = 25\n",
    "budget = 1000  # Set the number of function evaluations (budget)\n",
    "\n",
    "# Loop over each sample in the subset of synthetic data\n",
    "for idx, row in df_test.iterrows():\n",
    "    print(f\"Processing sample {idx + 1}/{len(df_test)}\")\n",
    "\n",
    "    # Extract the target data for this sample\n",
    "    target_Qfwd = np.array(row[\"log_Qfwd\"]).reshape(1, -1)\n",
    "    target_Qback = np.array(row[\"log_Qback\"]).reshape(1, -1)\n",
    "\n",
    "    # Apply MinMax scaling to the target\n",
    "    target_Qfwd_transformed = scaler_Qfwd.transform(target_Qfwd)\n",
    "    target_Qback_transformed = scaler_Qback.transform(target_Qback)\n",
    "    obj_fun = lambda params: objective_function(\n",
    "        params, target_Qfwd_transformed, target_Qback_transformed\n",
    "    )\n",
    "\n",
    "    # Initialize Differential Evolution optimizer with specified crossover and population size\n",
    "    optim_algo = ng.families.DifferentialEvolution(\n",
    "        crossover=\"twopoints\", popsize=num_workers\n",
    "    )\n",
    "\n",
    "    # Run the optimization process for this sample\n",
    "    optimizer = optim_algo(parametrization, budget=budget)\n",
    "\n",
    "    # start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Use a ThreadPoolExecutor for parallel batch processing\n",
    "    with futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        recommendation = optimizer.minimize(obj_fun, executor=executor)\n",
    "\n",
    "    # end time\n",
    "    end_time = time.time()\n",
    "    sample_runtime = end_time - start_time\n",
    "\n",
    "    # Append runtime and best geometry to the results\n",
    "    sample_runtimes.append({\"Sample\": idx + 1, \"Runtime (seconds)\": sample_runtime})\n",
    "    print(f\"Runtime for sample {idx}: {sample_runtime:.2f} seconds\")\n",
    "\n",
    "    best_geometry = recommendation.value\n",
    "    best_loss = recommendation.loss\n",
    "    print(f\"Best geometry for sample {idx}: {best_geometry}\")\n",
    "    print(f\"Best loss for sample {idx}: {best_loss}\")\n",
    "    print(\"-------------------------------------------------------\")\n",
    "\n",
    "    # Append results to the list\n",
    "    result = {\n",
    "        \"Sample Index\": idx + 1,\n",
    "        \"Best Geometry\": best_geometry,\n",
    "        \"Loss\": best_loss,\n",
    "        \"Runtime (seconds)\": sample_runtime,\n",
    "    }\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "\"\"\"\n",
    "##########################################################################################\n",
    "####################### Convert the runtimes into a DataFrame ############################\n",
    "##########################################################################################\n",
    "\"\"\"\n",
    "# Convert the results and runtimes to DataFrames and save\n",
    "results_df = pd.DataFrame(results)\n",
    "runtime_df = pd.DataFrame(sample_runtimes)\n",
    "\n",
    "# Optionally, save the DataFrames to files\n",
    "sample_runtime_path = \"runtime/global/01_testSetNew_122500_runtime_2500.pkl\"\n",
    "result_df_path = \"best_geometries/global/02_best_geo_testsetNew_122500_df_2500.pkl\"\n",
    "\n",
    "with open(sample_runtime_path, \"wb\") as f:\n",
    "    pickle.dump(runtime_df, f)\n",
    "\n",
    "with open(result_df_path, \"wb\") as f:\n",
    "    pickle.dump(results_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "##########################################################################################\n",
    "####################### save the best geo DataFrame ######################################\n",
    "##########################################################################################\n",
    "'''\n",
    "\n",
    "with open('best_geometries/global/02_best_geo_testsetNew_122500_df_2500.pkl', 'rb') as f:\n",
    "    best_df = pickle.load(f)\n",
    "    \n",
    "best_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'Best Geometry' column into multiple columns\n",
    "best_df[[\"r_core\", \"r_shell\", \"mat_core\", \"mat_shell\"]] = pd.DataFrame(\n",
    "    best_df[\"Best Geometry\"].tolist(), index=best_df.index\n",
    ")\n",
    "\n",
    "# Drop the 'Best Geometry' column if you no longer need it\n",
    "best_df = best_df.drop(columns=[\"Best Geometry\"])\n",
    "best_df = best_df[\n",
    "    [\n",
    "        \"mat_core\",\n",
    "        \"mat_shell\",\n",
    "        \"r_core\",\n",
    "        \"r_shell\",\n",
    "        \"Sample Index\",\n",
    "        \"Runtime (seconds)\",\n",
    "        \"Loss\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "best_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "###########################################################################################\n",
    "Now use the optimized geometries to calculate Mie Qfwd and Qback using  Mie theory function\n",
    "###########################################################################################\n",
    "\"\"\"\n",
    "\n",
    "# wavelengths = np.linspace(400, 800, 128)  # From 400 nm to 800 nm\n",
    "wavelengths = np.linspace(400, 800, 64)  # From 400 nm to 800 nm\n",
    "n_env = 1.0\n",
    "\n",
    "mie_Qfwd_list, mie_Qback_list = [], []\n",
    "# Loop through each row in the optimized geometries DataFrame\n",
    "for idx, row in best_df.iterrows():\n",
    "    mat_core = get_material(row[\"mat_core\"])\n",
    "    mat_shell = get_material(row[\"mat_shell\"])\n",
    "    r_core = row[\"r_core\"]\n",
    "    r_shell = row[\"r_shell\"]\n",
    "\n",
    "    # Calculate the Mie spectrum for this geometry\n",
    "    _, Qback, Qfwd = get_Mie_spec(\n",
    "        wavelengths, r_core, r_shell, mat_core, mat_shell, n_env\n",
    "    )\n",
    "\n",
    "    mie_Qfwd_list.append(Qfwd)\n",
    "    mie_Qback_list.append(Qback)\n",
    "\n",
    "# Convert the Mie Qfwd and Qback to DataFrame or arrays if needed\n",
    "mie_Qfwd_array = np.array(mie_Qfwd_list)\n",
    "mie_Qback_array = np.array(mie_Qback_list)\n",
    "\n",
    "# Validation: Check dimensions\n",
    "assert len(mie_Qfwd_array) == len(best_df), \"Mismatch in dimensions for Qfwd!\"\n",
    "assert len(mie_Qback_array) == len(best_df), \"Mismatch in dimensions for Qback!\"\n",
    "\n",
    "# Add the Mie results to the DataFrame\n",
    "best_df[\"mie_Qfwd\"] = mie_Qfwd_array.tolist()\n",
    "best_df[\"mie_Qback\"] = mie_Qback_array.tolist()\n",
    "\n",
    "best_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also compare Mie to foward model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the forward model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the own resblock class\n",
    "\n",
    "keras requires custom classes to be defined for being able to reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.utils.register_keras_serializable()\n",
    "class ResBlock1D(keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, convblock=False, **kwargs):\n",
    "        super(ResBlock1D, self).__init__(**kwargs)\n",
    "\n",
    "        # setup all necessary layers\n",
    "        self.conv1 = keras.layers.Conv1D(filters, kernel_size, padding=\"same\")\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = keras.layers.Conv1D(filters, kernel_size, padding=\"same\")\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "\n",
    "        self.relu = keras.layers.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.convblock = convblock\n",
    "        if self.convblock:\n",
    "            self.conv_shortcut = keras.layers.Conv1D(filters, 1)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        # add shortcut. optionally pass it through a Conv\n",
    "        if self.convblock:\n",
    "            x_sc = self.conv_shortcut(input_tensor)\n",
    "        else:\n",
    "            x_sc = input_tensor\n",
    "        x += x_sc\n",
    "        return self.relu(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"convblock\": self.convblock,\n",
    "            **base_config,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m forward_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/resnet/newData_122500/04_keras3_increaseBatch_decayLR_hybridMode.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;32m----> 2\u001b[0m forward_model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(forward_path)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "forward_path = \"models/resnet/newData_122500/04_keras3_increaseBatch_decayLR_hybridMode.keras\"\n",
    "forward_model = keras.models.load_model(forward_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#########################################################################################\n",
    "############### Use forward model to predict on the optimized geometries  ###############\n",
    "#########################################################################################\n",
    "\"\"\"\n",
    "\n",
    "# Ensure that the columns are ordered the same way as the input during training\n",
    "categorical_features = [\n",
    "    \"mat_core\",\n",
    "    \"mat_shell\",\n",
    "]  # Update these names based on your data\n",
    "numerical_features = [\"r_core\", \"r_shell\"]\n",
    "\n",
    "# Use the preprocessor to transform the data\n",
    "X_optimized_preprocessed = preprocessor.transform(\n",
    "    best_df[categorical_features + numerical_features]\n",
    ")\n",
    "\n",
    "# Predict using the forward model\n",
    "y_pred = forward_model.predict(X_optimized_preprocessed)\n",
    "\n",
    "# Separate predictions for Qfwd and Qback\n",
    "y_pred_Qfwd = y_pred[..., 0]  # First output for Qfwd predictions\n",
    "y_pred_Qback = y_pred[..., 1]  # Second output for Qback predictions\n",
    "\n",
    "# Inverse transform the predicted Qfwd and Qback to get the original scale\n",
    "predicted_Qfwd_original = scaler_Qfwd.inverse_transform(y_pred_Qfwd)\n",
    "predicted_Qback_original = scaler_Qback.inverse_transform(y_pred_Qback)\n",
    "\n",
    "# Apply expm1 to reverse the log1p transformation\n",
    "predicted_Qfwd_original = np.expm1(predicted_Qfwd_original)  # Reverse log1p for Qfwd\n",
    "predicted_Qback_original = np.expm1(predicted_Qback_original)  # Reverse log1p for Qback\n",
    "\n",
    "# Display the predicted Qfwd and Qback in original scale\n",
    "print(\"Predicted Qfwd (Original Scale):\", predicted_Qfwd_original)\n",
    "print(\"Predicted Qback (Original Scale):\", predicted_Qback_original)\n",
    "\n",
    "# Add the predicted values (in original scale) to the DataFrame\n",
    "best_df[\"predicted_Qfwd\"] = predicted_Qfwd_original.tolist()\n",
    "best_df[\"predicted_Qback\"] = predicted_Qback_original.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df['log_Qfwd'] = best_df['mie_Qfwd'].apply(lambda x: np.log1p(np.array(x)))\n",
    "best_df['log_Qback'] = best_df['mie_Qback'].apply(lambda x: np.log1p(np.array(x)))\n",
    "best_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#########################################################################################\n",
    "############### Save the Best Geometries, predicted, Mie into Dataframe   ###############\n",
    "#########################################################################################\n",
    "\"\"\"\n",
    "\n",
    "# save final_best_geometries_df to pkl file\n",
    "best_geometries_path = (\n",
    "    \"best_geometries/global/02_best_geo_testsetNew_122500_df_predicted_mie_2500.pkl\"\n",
    ")\n",
    "\n",
    "# Saving DataFrames\n",
    "with open(best_geometries_path, \"wb\") as f:\n",
    "    pickle.dump(best_df, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pykeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
