{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient-based Inverse Design Maximize Qfwd while Minimize Qback\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> Import modules <u/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-19 09:26:35.775396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742372795.792669 3026030 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742372795.797817 3026030 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-19 09:26:35.815776: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "from wgangp_model import load_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set dynamic GPU memory growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available and set to memory growth mode.\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU available\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU is available and set to memory growth mode.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU detected by TensorFlow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the own resblock class\n",
    "\n",
    "keras requires custom classes to be defined for being able to reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator to register the custom resblock to allow serialziation and re-loading\n",
    "@keras.utils.register_keras_serializable()  # for keras3\n",
    "class ResBlock1D(keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, convblock=False, **kwargs):\n",
    "        super(ResBlock1D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # setup all necessary layers\n",
    "        self.conv1 = keras.layers.Conv1D(filters, kernel_size, padding=\"same\")\n",
    "        self.bn1 = keras.layers.BatchNormalization()\n",
    "\n",
    "        self.conv2 = keras.layers.Conv1D(filters, kernel_size, padding=\"same\")\n",
    "        self.bn2 = keras.layers.BatchNormalization()\n",
    "\n",
    "        # self.relu = keras.layers.LeakyReLU()\n",
    "        self.relu = keras.layers.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.convblock = convblock\n",
    "        if self.convblock:\n",
    "            self.conv_shortcut = keras.layers.Conv1D(filters, 1)\n",
    "\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv1(input_tensor)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        # add shortcut. optionally pass it through a Conv\n",
    "        if self.convblock:\n",
    "            x_sc = self.conv_shortcut(input_tensor)\n",
    "        else:\n",
    "            x_sc = input_tensor\n",
    "        x += x_sc\n",
    "        return self.relu(x)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            \"convblock\": self.convblock,\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            **base_config,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reload the forward and wgangp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742372797.895909 3026030 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7030 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:21:00.0, compute capability: 8.9\n",
      "WARNING:absl:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "forward_path = \"models/resnet_Mie_predictor.keras\"\n",
    "wgangp_path = \"models/wgangp_generator.h5\"\n",
    "\n",
    "forward_model = keras.models.load_model(forward_path)\n",
    "generator = load_generator(wgangp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavelengths:[400.         406.34920635 412.6984127  419.04761905 425.3968254\n",
      " 431.74603175 438.0952381  444.44444444 450.79365079 457.14285714\n",
      " 463.49206349 469.84126984 476.19047619 482.53968254 488.88888889\n",
      " 495.23809524 501.58730159 507.93650794 514.28571429 520.63492063\n",
      " 526.98412698 533.33333333 539.68253968 546.03174603 552.38095238\n",
      " 558.73015873 565.07936508 571.42857143 577.77777778 584.12698413\n",
      " 590.47619048 596.82539683 603.17460317 609.52380952 615.87301587\n",
      " 622.22222222 628.57142857 634.92063492 641.26984127 647.61904762\n",
      " 653.96825397 660.31746032 666.66666667 673.01587302 679.36507937\n",
      " 685.71428571 692.06349206 698.41269841 704.76190476 711.11111111\n",
      " 717.46031746 723.80952381 730.15873016 736.50793651 742.85714286\n",
      " 749.20634921 755.55555556 761.9047619  768.25396825 774.6031746\n",
      " 780.95238095 787.3015873  793.65079365 800.        ]\n",
      "Target wavelength index 48 (704.7619047619048 nm)\n"
     ]
    }
   ],
   "source": [
    "wavelengths = np.linspace(400, 800, 64)\n",
    "print(f\"wavelengths:{wavelengths}\")\n",
    "target_lambda_index = np.argmin(np.abs(wavelengths - 705))\n",
    "\n",
    "print(\n",
    "    f\"Target wavelength index {target_lambda_index} ({wavelengths[target_lambda_index]} nm)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximize Qfwd Minimize Qback Fitness function\n",
    "---\n",
    "Add Series Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function_weight_series(\n",
    "    z_batch, generator, forward_model, target_lambda_index, weight_Qback\n",
    "):\n",
    "    synthetic_geometries = generator(z_batch)\n",
    "    synthetic_geometries_concat = keras.ops.concatenate(synthetic_geometries, axis=1)\n",
    "\n",
    "    # Forward pass through the forward model\n",
    "    predicted_batch = forward_model(synthetic_geometries_concat)\n",
    "\n",
    "    # Split output from the forward model\n",
    "    predicted_Qfwd_batch = predicted_batch[..., 0]  # Qfwd is the first channel\n",
    "    predicted_Qback_batch = predicted_batch[..., 1]  # Qback is the second channel\n",
    "\n",
    "    # Extract values at the target wavelength index\n",
    "    fitness_fwd = -predicted_Qfwd_batch[:, target_lambda_index]  # Maximize Qfwd\n",
    "    fitness_back = predicted_Qback_batch[:, target_lambda_index] * weight_Qback\n",
    "    # Combine to get total loss\n",
    "    total_loss_batch = fitness_fwd + fitness_back\n",
    "    return total_loss_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_latent_vector_weight_series_parallel(\n",
    "    z_batch,\n",
    "    generator,\n",
    "    forward_model,\n",
    "    target_lambda_index,\n",
    "    weight_Qback,\n",
    "    initial_lr=0.01,\n",
    "    iterations=250,\n",
    "    learning_rates=None,\n",
    "):\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=initial_lr)\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Calculate the loss for the batch of latent vectors\n",
    "            total_loss_batch = objective_function_weight_series(\n",
    "                z_batch, generator, forward_model, target_lambda_index, weight_Qback\n",
    "            )\n",
    "        gradients_batch = tape.gradient(total_loss_batch, [z_batch])\n",
    "        optimizer.apply_gradients(zip(gradients_batch, [z_batch]))\n",
    "\n",
    "        mean_loss = tf.reduce_mean(total_loss_batch).numpy()\n",
    "        loss_history.append(total_loss_batch.numpy())\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}, Mean Loss: {mean_loss:.6f}\")\n",
    "\n",
    "    final_loss = tf.reduce_mean(total_loss_batch).numpy()\n",
    "\n",
    "    return z_batch, final_loss, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Optimization Weight Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "batch_size = 500\n",
    "latent_dim = 128\n",
    "\n",
    "weight_series = [0, 0.5, 1, 5]\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "iterations = 250\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running optimization with lr= 0.01, weight 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1742372801.309330 3026030 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-03-19 09:26:41.915637: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.07GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2025-03-19 09:26:41.957400: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Mean Loss: -0.006276\n",
      "Iteration 50, Mean Loss: -0.466115\n",
      "Iteration 100, Mean Loss: -0.489592\n",
      "Iteration 150, Mean Loss: -0.500964\n",
      "Iteration 200, Mean Loss: -0.507533\n",
      "\n",
      "Running optimization with lr= 0.01, weight 0.5\n",
      "Iteration 0, Mean Loss: -0.213133\n",
      "Iteration 50, Mean Loss: -0.736208\n",
      "Iteration 100, Mean Loss: -0.759162\n",
      "Iteration 150, Mean Loss: -0.767369\n",
      "Iteration 200, Mean Loss: -0.771031\n",
      "\n",
      "Running optimization with lr= 0.01, weight 1\n",
      "Iteration 0, Mean Loss: -0.506933\n",
      "Iteration 50, Mean Loss: -1.132969\n",
      "Iteration 100, Mean Loss: -1.162334\n",
      "Iteration 150, Mean Loss: -1.172055\n",
      "Iteration 200, Mean Loss: -1.177228\n",
      "\n",
      "Running optimization with lr= 0.01, weight 5\n",
      "Iteration 0, Mean Loss: -2.692080\n",
      "Iteration 50, Mean Loss: -4.540533\n",
      "Iteration 100, Mean Loss: -4.622373\n",
      "Iteration 150, Mean Loss: -4.653134\n",
      "Iteration 200, Mean Loss: -4.668675\n",
      "\n",
      "Weight: 0, Learning Rate: 0.01\n",
      "Final Loss: -0.5119\n",
      "\n",
      "Weight: 0.5, Learning Rate: 0.01\n",
      "Final Loss: -0.7731\n",
      "\n",
      "Weight: 1, Learning Rate: 0.01\n",
      "Final Loss: -1.1821\n",
      "\n",
      "Weight: 5, Learning Rate: 0.01\n",
      "Final Loss: -4.6884\n"
     ]
    }
   ],
   "source": [
    "# Loop through each weight in the weight series\n",
    "for weight_Qback in weight_series:\n",
    "    print(f\"\\nRunning optimization with lr= {initial_learning_rate}, weight {weight_Qback}\")\n",
    "\n",
    "    initial_z_batch = np.random.normal(size=(batch_size, latent_dim)) * 2\n",
    "    z_batch_tf = tf.Variable(initial_z_batch, dtype=tf.float32)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Optimize the latent vectors for the current weight\n",
    "    optimized_z_batch, final_loss, loss_history = (\n",
    "        optimize_latent_vector_weight_series_parallel(\n",
    "            z_batch_tf,\n",
    "            generator,\n",
    "            forward_model,\n",
    "            target_lambda_index,\n",
    "            weight_Qback,\n",
    "            initial_lr=initial_learning_rate,\n",
    "            iterations=iterations,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    # Store the optimized latent vectors and final loss for this weight\n",
    "    results.append(\n",
    "        {\n",
    "            \"weight_Qback\": weight_Qback,\n",
    "            \"learning_rate\": initial_learning_rate,\n",
    "            \"optimized_z_batch\": optimized_z_batch,\n",
    "            \"final_loss\": final_loss,\n",
    "            \"loss_history\": loss_history,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Print summary of results\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"\\nWeight: {result['weight_Qback']}, Learning Rate: {result['learning_rate']}\"\n",
    "    )\n",
    "    print(f\"Final Loss: {result['final_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"best_geometries/gradient_max_min_problem_lrfixed.pkl\", \"wb\") as pickle_file:\n",
    "    pickle.dump(results, pickle_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".pykeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
